---
title: "Machine Learning Final Project"
author: "Santiago Puentes Navas"
date: "8/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(caret)
library(lattice)
library(ggplot2)
library(gbm)
library(e1071)
library(randomForest)
```

## Introduction

In this project I have chosen to perform ML analysis using the caret package (v.6.0). First, I will use pre-processing tools to improve the model upon which I will build the algorithms, this due to the fact that the dataset contains a high number of variables, many of which differ in orders of magnitude (this can generate a problem being in different scales), as well as the distributions of the variables. Certain variables will also be excluded from the model building phase, namely, people names, time windows and timestamps; in this analysis we will not focus on the time window, and instead, we will focus on other numeric/factor variables in order to predict the "performance" outcome, in the *classe* variable. We will use Cross-Validation to improve the model, as well as 

## First Steps

After loading the required datafiles through R, we will start by cleaning those variables that are irrelevant in the test set, i.e. Those values that only contain NA's and thus will not work with out built models. Afterwards we will clean both datasets of the aforementioned variables, those which are not relevant for analysis (names, time-related variables).

```{r firststeps}
PMLtraining = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
PMLtesting = read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

exclude_prmtrs = sapply(PMLtesting,is.logical)

PMLtraining = PMLtraining[,!exclude_prmtrs]
PMLtesting = PMLtesting[,!exclude_prmtrs]

PMLtraining = PMLtraining[, grepl("accel|classe",names(PMLtraining))]
PMLtesting = PMLtesting[, grepl("accel|classe",names(PMLtesting))]



```

## Pre-Processing

Then, after performing the initial cleaning of the training and test sets (remember, both must be subjected to the same transformations and procedures), we will perform pre-processing using the adequate function. We will normalize (centering and scaling) variables, as well as get rid of those "Near-Zero variance" variables. In this project, no PCA will be done for the sake of explaining the final model's variables and results and to save computational time.

```{r preprocess}
preProcTrain = preProcess(PMLtraining, method = c("nzv", "corr"))
preProcTest = preProcess(PMLtesting, method = c("nzv", "corr"))

PMLtraining = predict(preProcTrain, PMLtraining)
PMLtesting = predict(preProcTrain, PMLtesting)
```

## Cross-Validation

For this project, Cross-Validation is included into the training phase by using the **trainControl** function, for this function, we will use k-fold Cross-Validation, using 10 folds in order to reduce the prediction error. Since using this parameter is time consuming when we introduce it into our ML training functions, this is one of the reasons PCA was not used previously as a pre-processing factor.

```{r cvalidation}
TrainingCV <- trainControl(method = "cv", number = 10)
```

## Training Algorithms

For the the purpose of a broader perspective, in this model I will use three algorithms: Stochastic Gradient Boosting ("gbm"), Random Forests ("rf") and Support Vector Machine ("svmLinear2"). We define a default seed (12345) and begin training the models.

```{r training, cache = TRUE}
set.seed(12345)
firstModel <- train(classe ~ ., data = PMLtraining, method = "gbm" ,trControl = TrainingCV, verbose =  FALSE)
secondModel <- train(classe ~ ., data = PMLtraining, method = "rf" ,trControl = TrainingCV)
thirdModel <- train(classe ~ ., data = PMLtraining, method = "svmLinear2" ,trControl = TrainingCV)
```

## Results & Conclusion

For all three models, the code below indicates that all of the 20 observations are labeled as category "A"; since in this exercise we do not have the true labels to compare against, it is difficult to have a good measurement of error and other important metrics (precision, recall). Afterwards, the plots for the three models are shown. For the "gbm" algorithm, the CV accuracy is plotted against the number of boosting iterations. For the "rf" one, the CV accuracy is ploted against the number of selected predictors. Finally, for the "svmLinear2", the same metric (CV accuracy) is plotted vs. the Cost (a term that indicates penalization of the predictors)

```{r results, cache = TRUE, echo=FALSE,results='hide', message = FALSE}
firstRes = predict(firstModel, PMLtesting)
plot(firstModel)
secondRes = predict(secondModel, PMLtesting)
plot(secondModel)
thirdRes = predict(thirdModel, PMLtesting)
plot(thirdModel)
```

And now using the true results of the classes, we can compare using confusionMatrix, in order to obtain the result for the better prediction algorithm:

```{r results2, cache = TRUE}
trueResult = as.factor(c("B","A","B","A","A","E","D","B","A","A","B","C","B","A","E","E","A","B","B","B"))

confusionMatrix(firstRes,trueResult)
confusionMatrix(secondRes,trueResult)
confusionMatrix(thirdRes,trueResult)
```

In conclusion, random forests was a really good algorithm (95% accuracy) when compared to Gradient Boosting (75% accuracy) and SVMs (only 60% accuracy). This implies, as stated by the book "Elements of Statistical Learning" that Random Forests are good predicting when it comes to categorical multiclass output variables vs. multiple numeric predictor variables.
